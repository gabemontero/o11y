evaluation_interval: 1m

rule_files:
  - prometheus.pipeline_alerts.yaml

tests:
  # ----- Chains Controller Performance / High Overhead ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="1", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="10", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="+Inf", source_cluster="cluster01"}'
        values: '900+900x780'
      - series: 'watcher_client_latency_bucket{app="tekton-pipelines-controller", le="1", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'watcher_client_latency_bucket{app="tekton-pipelines-controller", le="+Inf", source_cluster="cluster01"}'
        values: '1x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ChainsControllerPerformance
        exp_alerts:
          - exp_labels:
              severity: normal
              slo: "false"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton chains controller performance appears degraded with uncommonly high client latency.
              description: >-
                Tekton chains controller on cluster cluster01 performance appears degraded with client latency 33.33%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/chains-performance.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="10", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="+Inf", source_cluster="cluster01"}'
        values: '400+400x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ChainsControllerPerformance

  # ----- Results Too Many Errors ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton results is experiencing too many API errors.
              description: >-
                Tekton results on cluster cluster01 only has an API success rate of 25%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/tekton-result-high-errors.md
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton results is experiencing too many API errors.
              description: >-
                Tekton results on cluster cluster01 only has an API success rate of 50%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/tekton-result-high-errors.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '100+100x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors

  # ----- PAC Controller Performance / High Overhead ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'pac_watcher_client_latency_bucket{le="1", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pac_watcher_client_latency_bucket{le="10", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pac_watcher_client_latency_bucket{le="+Inf", source_cluster="cluster01"}'
        values: '900+900x780'
      - series: 'pac_watcher_client_latency_bucket{le="1", source_cluster="cluster02"}'
        values: '1x780'
      - series: 'pac_watcher_client_latency_bucket{le="+Inf", source_cluster="cluster02"}'
        values: '1x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: PACControllerPerformance
        exp_alerts:
          - exp_labels:
              severity: normal
              slo: "false"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton PAC controller performance appears degraded with uncommonly high client latency.
              description: >-
                Tekton PAC controller on cluster cluster01 performance appears degraded with client latency 33.33%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/pac-performance.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'pac_watcher_client_latency_bucket{le="1", source_cluster="cluster02"}'
        values: '300+300x780'
      - series: 'pac_watcher_client_latency_bucket{le="+Inf", source_cluster="cluster02"}'
        values: '400+400x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: PACControllerPerformance
